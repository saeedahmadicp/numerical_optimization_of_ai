# algorithms/convex/configs/steepest_descent.yaml

# Configuration for Steepest Descent Method - primarily for optimization

# Root-finding configuration (using gradient-based approach)
root:
  # Function with known roots
  function: cubic_polynomial  # f(x) = x^3 - 2x^2 - 5x + 6, roots at x = {-2, 1, 3}
  
  # Initial point
  x0: 2.0
  
  # Algorithm parameters
  tol: 1.0e-8
  max_iter: 100
  
  # Step size control
  step_size: 0.1  # Fixed step size
  
  # Expected outcome
  expected_root: 3.0
  
  # Visualization settings
  xrange: [-3, 4]
  no_viz: true

# Optimization configuration
optimize:
  # Function with known minimum
  function: quadratic  # f(x) = x^2 + 4x + 4, minimum at x = -2
  
  # Initial point
  x0: 0.0
  
  # Algorithm parameters
  tol: 1.0e-8
  max_iter: 100
  
  # Step length parameters
  step_length_method: exact_line_search  # Exact line search for quadratics
  
  # Expected outcome
  expected_minimum: -2.0
  
  # Visualization settings
  xrange: [-4, 2]
  no_viz: true

# Optimization with backtracking
optimize_backtracking:
  # Function with known minimum
  function: exponential_bowl  # f(x) = e^(x^2) - 1, minimum at x = 0
  
  # Initial point
  x0: 1.0
  
  # Algorithm parameters
  tol: 1.0e-8
  max_iter: 200  # May require more iterations
  
  # Step length parameters
  step_length_method: backtracking
  step_length_params: {"alpha_init": 1.0, "rho": 0.5, "c": 1.0e-4}
  
  # Expected outcome
  expected_minimum: 0.0
  
  # Visualization settings
  xrange: [-2, 2]
  no_viz: true

# For 2D optimization
optimize_2d:
  # Function to optimize
  function: quadratic_bowl  # f(x,y) = x^2 + y^2, minimum at (0,0)
  
  # Initial point - starting away from minimum
  x0: [1.0, 1.0]
  
  # Algorithm parameters
  tol: 1.0e-8
  max_iter: 100
  
  # Step length parameters
  step_length_method: constant
  step_length_params: {"alpha": 0.1}
  
  # Expected outcome
  expected_minimum: [0.0, 0.0]
  
  # Visualization settings
  xrange: [-2, 2]
  yrange: [-2, 2]
  viz_3d: true
  no_viz: true

# 2D optimization with challenging function
optimize_rosenbrock:
  # Function to optimize - Rosenbrock function (challenging for steepest descent)
  function: rosenbrock  # f(x,y) = (1-x)^2 + 100(y-x^2)^2, minimum at (1,1)
  
  # Initial point - starting away from minimum
  x0: [0.0, 0.0]
  
  # Algorithm parameters
  tol: 1.0e-8
  max_iter: 10000  # Many iterations likely needed
  
  # Step length parameters
  step_length_method: armijo
  step_length_params: {"alpha_init": 1.0, "beta": 0.5, "c": 1.0e-4}
  
  # Expected outcome
  expected_minimum: [1.0, 1.0]
  
  # Visualization settings
  xrange: [-1, 2]
  yrange: [-1, 2]
  viz_3d: true
  no_viz: true 